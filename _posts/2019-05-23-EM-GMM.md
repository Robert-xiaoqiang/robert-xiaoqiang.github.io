---
title: EM-GMM
key: 201905230
tags: Unsurprised MLE 隐变量迭代 变分
---

# EM-GMM

- 无监督学习
  - 前提 数据分布假设
  - 目标 数据分布拟合, 分布规律或分布模式的拟合



<!--more-->



- 引入隐变量GM的问题概括为

- $$
  p(X, Z|\theta) 是假设分布 \\
  为什么不按照一般假设p(X|\theta) ?\\
  显然GDA, MLE, Bayesian Parameter Estimation, Class Conditional Density, nabula都不work, SGD也不work\\
  Z == \pi_k \\
  \theta == (\mu_k, \Sigma_k)^T \\
  我们的目标(也是GM的原始目标)是 \\
  \theta^* = argmax\{ \sum_n^N{ln(p(X|\theta))}\} \\
  实际算法只对ln(p(X|\theta))进行下界的不断优化
  $$

- EM算法的通用性证明概要

- 证明1, 直接将边际分布作为联合分布的积分, *Jensen*不等式优化, 计算L(theta) - L(theta_old) 得优化项形如

- $$
  L(\theta^{old}) + \sum_Z{p(Z|X, \theta^{old})\frac{p(X, Z|\theta)}{p(X, Z|\theta^{old})}}
  $$

- 证明2, 直接将似然函数乘上归一化概率分布q(z), 即p(z|x, theta_old), 得到KL diverge和一项可优化的下界

- $$
  q(Z) = p(Z|X, \theta^{old})\\
  KL(q||p) + \sum_Z{q(Z)\frac{p(X, Z|\theta)}{q(Z)}}
  $$

- 省略不可优化项, 二者可化简为期望式

  

## EM overview

- 
  $$
  p(Z|X, \theta^{old}) \\
  argmax\{E_z(ln(p(X, Z|\pi_k^{old}, \mu_k^{old}, \Sigma_k^{old})))\} \\
  update \theta
  $$

## EM in GMM

- 上述期望式将混合系数, 高斯均值, 高斯协方差矩阵完美分离
- p(z|x, theta_old)可以转化为贝叶斯公式计算, 混合系数是先验
- E步骤, 算后验, 算响应因子而已; M部最大化期望, 最大化下界而已

$$
符号约定 \\
p(Z) == z_n^k == \pi_k \\
\gamma(z_n^k) == p(Z|X, \theta^{old})
$$

## 我理解这个过程超过20小时, 谢谢